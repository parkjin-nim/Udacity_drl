{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suitable-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "maritime-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "OBS_DIM = ENV.observation_space.shape[0]\n",
    "ACT_DIM = ENV.action_space.shape[0]\n",
    "\n",
    "ACT_LIMIT = ENV.action_space.high[0]\n",
    "ENV.close()\n",
    "\n",
    "GAMMA = 0.99\n",
    "SEED = 300 #{1,100,200,300,600}\n",
    "\n",
    "hidden_size = 256                                          \n",
    "lr          = 3e-4\n",
    "num_steps   = 32\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-prompt",
   "metadata": {},
   "source": [
    "## for Mac OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parental-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(episode, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(40,10))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, rewards[-1]))\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('rewards')\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "use_cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-baseball",
   "metadata": {},
   "source": [
    "## Multiprocessing env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-subscriber",
   "metadata": {},
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 1\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-attention",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joined-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, seed=100, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.value = self.critic(x)\n",
    "        self.mu    = self.actor(x)        \n",
    "        self.std   = self.log_std.exp().expand_as(self.mu)      #\n",
    "        self.dist  = Normal(self.mu, self.std)\n",
    "        return self.dist, self.value\n",
    "    \n",
    "    # Actor를 이용해 state를 받아서 action을 예측, 반환\n",
    "    def get_action(self, x):\n",
    "        return  # TODO\n",
    "    \n",
    "    def learn(self, state_lst, logprob_lst, q_val_lst, entropy, optimizer):\n",
    "        \"\"\"\n",
    "\n",
    "            Computes advantages by subtracting a bseline(V(from critic)) from the estimated Q values\n",
    "            추가로 해볼 수 있는 것 : advantage normalize\n",
    "            Training a ActorCritic Agent refers to updating its actor using the given observations/actions\n",
    "            and the calculated q_values/ advantages that come from the seen rewards\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.log_probs = torch.cat(logprob_lst)\n",
    "        self.returns   = torch.cat(q_val_lst).detach()\n",
    "        self.values    = torch.cat(state_lst)\n",
    "        \n",
    "        self.advantage = self.returns - self.values\n",
    "              \n",
    "        self.actor_loss  = -(self.log_probs * self.advantage.detach()).mean()\n",
    "        self.critic_loss = self.advantage.pow(2).mean()\n",
    "\n",
    "        self.loss = self.actor_loss + 0.5 * self.critic_loss - 0.001 * entropy \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        optimizer.step()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-robin",
   "metadata": {},
   "source": [
    "## N-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_td(reward_lst, V):\n",
    "    q_val_lst = []\n",
    "    # TODO: n_step td return\n",
    "    R = V\n",
    "    for step in reversed(range(len(reward_lst))):  # TODO:\n",
    "        R = reward_lst[step] + GAMMA * R\n",
    "        q_val_lst.insert(0, R)\n",
    "    return q_val_lst\n",
    "\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broken-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Worker(num_episodes, n_steps):\n",
    "    env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "\n",
    "    agent = ActorCritic(OBS_DIM, ACT_DIM).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-4)  \n",
    "    \n",
    "    ####################################################\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    start_time = time.time()\n",
    "    epi_plot = []\n",
    "    finish = False\n",
    "    ####################################################\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        epi_reward = 0.\n",
    "        \n",
    "        while not done:\n",
    "            s_lst, a_lst, r_lst = [], [], []\n",
    "            entropy = 0 # \n",
    "            masks = []\n",
    "            \n",
    "            # N-step rollout\n",
    "            for t in range(n_steps):\n",
    "                # TODO\n",
    "                # action = agent.get_action # TODO\n",
    "                # while env takes in/out in numpy, nn.module does in tensor, convert!\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                dist, value = agent(state) \n",
    "                \n",
    "                action = dist.sample()\n",
    "                #####################################################\n",
    "                next_state, reward, done, _ = env.step(action.cpu().numpy()[0])\n",
    "                epi_reward += reward\n",
    "                #####################################################\n",
    "            \n",
    "                # TODO\n",
    "                log_prob = dist.log_prob(action)\n",
    "                entropy += dist.entropy().mean()\n",
    "                \n",
    "                a_lst.append(log_prob)\n",
    "                s_lst.append(value)\n",
    "                \n",
    "                # scalar reward, done to tensor\n",
    "                r_lst.append(torch.FloatTensor([reward]).unsqueeze(1).to(device))\n",
    "                #masks.append(torch.FloatTensor([1 - done]).unsqueeze(1).to(device))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = next_state\n",
    "\n",
    "            # HINT : if done -> V = 0, else -> V = agent.critic(last state of N-step rollout trajectory)\n",
    "            V =  0 if done else value # TODO            \n",
    "            q_val_lst = n_step_td(r_lst, V)\n",
    "            #q_val_lst = compute_gae(V, r_lst, masks, s_lst)\n",
    "                                  \n",
    "            agent.learn(s_lst, a_lst, q_val_lst, entropy, optimizer)\n",
    "        \n",
    "        \n",
    "        ######################################################################\n",
    "\n",
    "        episode_rewards.append(epi_reward)\n",
    "        \n",
    "        \n",
    "        if episode >= 100:\n",
    "            mean_100_episode_reward = np.mean(episode_rewards)\n",
    "            epi_plot.append(mean_100_episode_reward)\n",
    "            if episode % 10 == 0:\n",
    "                print(\"\\nEpisode: {}, avg score: {:.1f}\".format(episode, mean_100_episode_reward))\n",
    "\n",
    "            if mean_100_episode_reward >= 500:\n",
    "                finish = True\n",
    "                print(\"Solved (1)!!!, Time : {:.2f}\".format(time.time() - start_time))\n",
    "                np.save(\"./single.npy\", np.array(epi_plot))\n",
    "                return\n",
    "            \n",
    "            \n",
    "    env.close()\n",
    "    print(\"Fail... Retry\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-place",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 100, avg score: -1075.8\n",
      "\n",
      "Episode: 110, avg score: -1024.2\n",
      "\n",
      "Episode: 120, avg score: -1009.8\n",
      "\n",
      "Episode: 130, avg score: -961.4\n",
      "\n",
      "Episode: 140, avg score: -921.0\n",
      "\n",
      "Episode: 150, avg score: -882.3\n",
      "\n",
      "Episode: 160, avg score: -857.9\n",
      "\n",
      "Episode: 170, avg score: -813.7\n",
      "\n",
      "Episode: 180, avg score: -770.2\n",
      "\n",
      "Episode: 190, avg score: -735.9\n",
      "\n",
      "Episode: 200, avg score: -697.1\n",
      "\n",
      "Episode: 210, avg score: -655.4\n",
      "\n",
      "Episode: 220, avg score: -610.4\n",
      "\n",
      "Episode: 230, avg score: -580.4\n",
      "\n",
      "Episode: 240, avg score: -563.8\n",
      "\n",
      "Episode: 250, avg score: -548.3\n",
      "\n",
      "Episode: 260, avg score: -505.1\n",
      "\n",
      "Episode: 270, avg score: -483.3\n",
      "\n",
      "Episode: 280, avg score: -469.1\n",
      "\n",
      "Episode: 290, avg score: -432.9\n",
      "\n",
      "Episode: 300, avg score: -404.9\n",
      "\n",
      "Episode: 310, avg score: -381.7\n",
      "\n",
      "Episode: 320, avg score: -389.2\n",
      "\n",
      "Episode: 330, avg score: -409.1\n",
      "\n",
      "Episode: 340, avg score: -424.4\n",
      "\n",
      "Episode: 350, avg score: -402.0\n",
      "\n",
      "Episode: 360, avg score: -455.1\n",
      "\n",
      "Episode: 370, avg score: -471.0\n",
      "\n",
      "Episode: 380, avg score: -471.3\n",
      "\n",
      "Episode: 390, avg score: -476.4\n",
      "\n",
      "Episode: 400, avg score: -462.1\n",
      "\n",
      "Episode: 410, avg score: -505.3\n",
      "\n",
      "Episode: 420, avg score: -482.1\n",
      "\n",
      "Episode: 430, avg score: -459.3\n",
      "\n",
      "Episode: 440, avg score: -432.2\n",
      "\n",
      "Episode: 450, avg score: -409.0\n",
      "\n",
      "Episode: 460, avg score: -380.4\n",
      "\n",
      "Episode: 470, avg score: -368.6\n",
      "\n",
      "Episode: 480, avg score: -381.7\n",
      "\n",
      "Episode: 490, avg score: -352.1\n",
      "\n",
      "Episode: 500, avg score: -347.9\n",
      "\n",
      "Episode: 510, avg score: -304.4\n",
      "\n",
      "Episode: 520, avg score: -296.2\n",
      "\n",
      "Episode: 530, avg score: -262.6\n",
      "\n",
      "Episode: 540, avg score: -236.6\n",
      "\n",
      "Episode: 550, avg score: -225.4\n",
      "\n",
      "Episode: 560, avg score: -184.5\n",
      "\n",
      "Episode: 570, avg score: -128.0\n",
      "\n",
      "Episode: 580, avg score: -60.6\n",
      "\n",
      "Episode: 590, avg score: -92.2\n",
      "\n",
      "Episode: 600, avg score: -97.8\n",
      "\n",
      "Episode: 610, avg score: -73.4\n",
      "\n",
      "Episode: 620, avg score: -28.3\n",
      "\n",
      "Episode: 630, avg score: -14.4\n",
      "\n",
      "Episode: 640, avg score: -5.8\n",
      "\n",
      "Episode: 650, avg score: -6.2\n",
      "\n",
      "Episode: 660, avg score: 14.2\n",
      "\n",
      "Episode: 670, avg score: -90.1\n",
      "\n",
      "Episode: 680, avg score: -154.8\n",
      "\n",
      "Episode: 690, avg score: -164.1\n",
      "\n",
      "Episode: 700, avg score: -190.3\n",
      "\n",
      "Episode: 710, avg score: -174.8\n",
      "\n",
      "Episode: 720, avg score: -180.9\n",
      "\n",
      "Episode: 730, avg score: -159.0\n",
      "\n",
      "Episode: 740, avg score: -133.2\n",
      "\n",
      "Episode: 750, avg score: -108.7\n",
      "\n",
      "Episode: 760, avg score: -91.5\n",
      "\n",
      "Episode: 770, avg score: -6.0\n",
      "\n",
      "Episode: 780, avg score: 51.2\n",
      "\n",
      "Episode: 790, avg score: 91.3\n",
      "\n",
      "Episode: 800, avg score: 168.3\n",
      "\n",
      "Episode: 810, avg score: 131.5\n",
      "\n",
      "Episode: 820, avg score: 157.0\n",
      "\n",
      "Episode: 830, avg score: 146.1\n",
      "\n",
      "Episode: 840, avg score: 161.4\n",
      "\n",
      "Episode: 850, avg score: 49.2\n",
      "\n",
      "Episode: 860, avg score: 37.0\n",
      "\n",
      "Episode: 870, avg score: 90.9\n",
      "\n",
      "Episode: 880, avg score: 36.2\n",
      "\n",
      "Episode: 890, avg score: 5.6\n",
      "\n",
      "Episode: 900, avg score: -51.9\n",
      "\n",
      "Episode: 910, avg score: -11.1\n",
      "\n",
      "Episode: 920, avg score: -77.5\n",
      "\n",
      "Episode: 930, avg score: -71.9\n",
      "\n",
      "Episode: 940, avg score: -74.0\n",
      "\n",
      "Episode: 950, avg score: 64.9\n",
      "\n",
      "Episode: 960, avg score: 69.6\n",
      "\n",
      "Episode: 970, avg score: -5.2\n",
      "\n",
      "Episode: 980, avg score: 68.9\n",
      "\n",
      "Episode: 990, avg score: 145.6\n",
      "\n",
      "Episode: 1000, avg score: 209.2\n",
      "\n",
      "Episode: 1010, avg score: 218.8\n",
      "\n",
      "Episode: 1020, avg score: 262.7\n",
      "\n",
      "Episode: 1030, avg score: 279.8\n",
      "\n",
      "Episode: 1040, avg score: 279.3\n",
      "\n",
      "Episode: 1050, avg score: 174.0\n",
      "\n",
      "Episode: 1060, avg score: 217.6\n",
      "\n",
      "Episode: 1070, avg score: 238.3\n",
      "\n",
      "Episode: 1080, avg score: 181.2\n",
      "\n",
      "Episode: 1090, avg score: 207.3\n",
      "\n",
      "Episode: 1100, avg score: 223.9\n",
      "\n",
      "Episode: 1110, avg score: 235.6\n",
      "\n",
      "Episode: 1120, avg score: 252.1\n",
      "\n",
      "Episode: 1130, avg score: 174.9\n",
      "\n",
      "Episode: 1140, avg score: 211.0\n",
      "\n",
      "Episode: 1150, avg score: 266.2\n",
      "\n",
      "Episode: 1160, avg score: 222.4\n",
      "\n",
      "Episode: 1170, avg score: 284.0\n",
      "\n",
      "Episode: 1180, avg score: 340.4\n",
      "\n",
      "Episode: 1190, avg score: 149.8\n",
      "\n",
      "Episode: 1200, avg score: 57.1\n",
      "\n",
      "Episode: 1210, avg score: 49.0\n",
      "\n",
      "Episode: 1220, avg score: 30.3\n",
      "\n",
      "Episode: 1230, avg score: 89.4\n",
      "\n",
      "Episode: 1240, avg score: -3.4\n",
      "\n",
      "Episode: 1250, avg score: -24.4\n",
      "\n",
      "Episode: 1260, avg score: 11.2\n",
      "\n",
      "Episode: 1270, avg score: -6.3\n",
      "\n",
      "Episode: 1280, avg score: -28.6\n",
      "\n",
      "Episode: 1290, avg score: 147.5\n",
      "\n",
      "Episode: 1300, avg score: 139.5\n",
      "\n",
      "Episode: 1310, avg score: 43.0\n",
      "\n",
      "Episode: 1320, avg score: 87.9\n",
      "\n",
      "Episode: 1330, avg score: 140.2\n",
      "\n",
      "Episode: 1340, avg score: 154.5\n",
      "\n",
      "Episode: 1350, avg score: 165.3\n",
      "\n",
      "Episode: 1360, avg score: 139.3\n",
      "\n",
      "Episode: 1370, avg score: 108.7\n",
      "\n",
      "Episode: 1380, avg score: 182.8\n",
      "\n",
      "Episode: 1390, avg score: 172.0\n",
      "\n",
      "Episode: 1400, avg score: 281.8\n",
      "\n",
      "Episode: 1410, avg score: 375.2\n",
      "\n",
      "Episode: 1420, avg score: 356.4\n",
      "\n",
      "Episode: 1430, avg score: 343.3\n",
      "\n",
      "Episode: 1440, avg score: 429.0\n",
      "Solved (1)!!!, Time : 3399.63\n"
     ]
    }
   ],
   "source": [
    "def run(num_episodes):\n",
    "    n_steps = 5  # TODO up to you\n",
    "\n",
    "    Worker(num_episodes, n_steps)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-gather",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
