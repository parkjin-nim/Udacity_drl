{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "further-geometry",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-covering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "import pybullet_envs\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-garbage",
   "metadata": {},
   "source": [
    "## for Mac OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessible-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-virtue",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instant-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jinpark/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 12\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plain-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-synthetic",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "further-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, seed=100, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.value = self.critic(x)\n",
    "        self.mu    = self.actor(x)      \n",
    "        self.std   = self.log_std.exp().expand_as(self.mu)\n",
    "        self.dist  = Normal(self.mu, self.std)\n",
    "        return self.dist, self.value\n",
    "    \n",
    "    \n",
    "    def learn(self, state_lst, logprob_lst, q_val_lst, entropy, optimizer):\n",
    "        \n",
    "        self.log_probs = torch.cat(logprob_lst)\n",
    "        self.returns   = torch.cat(q_val_lst).detach()\n",
    "        self.values    = torch.cat(state_lst)\n",
    "        \n",
    "        self.advantage = self.returns - self.values\n",
    "              \n",
    "        self.actor_loss  = -(self.log_probs * self.advantage.detach()).mean()\n",
    "        self.critic_loss = self.advantage.pow(2).mean()\n",
    "\n",
    "        self.loss = self.actor_loss + 0.5 * self.critic_loss - 0.001 * entropy \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-livestock",
   "metadata": {},
   "source": [
    "## A2C: Synchronous Advantage Actor Critic\n",
    "\n",
    "[OpenAI Blog:](\"https://blog.openai.com/baselines-acktr-a2c/#a2canda3c\\\")\n",
    "\n",
    "The Asynchronous Advantage Actor Critic method (A3C) has been very influential since the paper was published. The algorithm combines a few key ideas:\n",
    "\n",
    "- An updating scheme that operates on fixed-length segments of experience (say, 20 timesteps) and uses these segments to compute estimators of the returns and advantage function.\n",
    "- Architectures that share layers between the policy and value function.\n",
    "- Asynchronous updates.\n",
    "\n",
    "After reading the paper, AI researchers wondered whether the asynchrony led to improved performance (e.g. “perhaps the added noise would provide some regularization or exploration?“), or if it was just an implementation detail that allowed for faster training with a CPU-based implementation.\n",
    "\n",
    "As an alternative to the asynchronous implementation, researchers found you can write a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before performing an update, averaging over all of the actors. One advantage of this method is that it can more effectively use of GPUs, which perform best with large batch sizes. This algorithm is naturally called A2C, short for advantage actor critic. (This term has been used in several papers.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joined-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input space: 26\n",
      "input space: 6\n",
      "high_limit: 1.0\n"
     ]
    }
   ],
   "source": [
    "state_size  = envs.observation_space.shape[0]\n",
    "action_size = envs.action_space.shape[0]\n",
    "high_limit = envs.action_space.high[0]\n",
    "print(\"input space:\", state_size)\n",
    "print(\"input space:\", action_size)\n",
    "print(\"high_limit:\", high_limit)\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size = 256                                         \n",
    "lr          = 1e-4\n",
    "num_steps   = 5\n",
    "num_episodes = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "registered-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_td(reward_lst, V, gamma=0.99):\n",
    "    q_val_lst = []\n",
    "    # TODO: n_step td return\n",
    "    R = V\n",
    "    for step in reversed(range(len(reward_lst))):  # TODO:\n",
    "        R = reward_lst[step] + gamma * R\n",
    "        q_val_lst.insert(0, R)\n",
    "    return q_val_lst\n",
    "\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "based-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = envs.reset()\n",
    "\n",
    "def Worker(num_episodes, n_steps):\n",
    "    #env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "\n",
    "    agent = ActorCritic(state_size, action_size).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-4) \n",
    "    \n",
    "    #####################################################\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    start_time = time.time()\n",
    "    epi_plot = []\n",
    "    finish = False\n",
    "    #####################################################\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        states = envs.reset()\n",
    "        scores = np.zeros(num_envs)                          # initialize the score (for each agent)\n",
    "        dones = np.zeros(num_envs, dtype=bool)\n",
    "        while not np.any(dones):\n",
    "\n",
    "            log_probs = []\n",
    "            values    = []\n",
    "            nstep_rewards   = []\n",
    "            masks     = []\n",
    "            entropy = 0\n",
    "\n",
    "            for step in range(n_steps):\n",
    "\n",
    "                dist, value = agent(torch.FloatTensor(states).to(device)) \n",
    "                actions = dist.sample()\n",
    "                \n",
    "                next_states, rewards, dones, _ = envs.step(actions.cpu().numpy())\n",
    "                scores += rewards                         \n",
    "                states = next_states                      \n",
    "             \n",
    "                log_prob = dist.log_prob(actions)\n",
    "                entropy += dist.entropy().mean()\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                nstep_rewards.append(torch.FloatTensor(rewards).unsqueeze(1).to(device))\n",
    "                masks.append(torch.FloatTensor(1 - dones).unsqueeze(1).to(device))\n",
    "\n",
    "                if np.any(dones):                                  # exit loop if episode finished\n",
    "                    break\n",
    "\n",
    "            _, next_values = agent(torch.FloatTensor(next_states).to(device))\n",
    "            next_values = torch.zeros(num_envs).unsqueeze(1) if np.any(dones) else next_values\n",
    "\n",
    "            returns = compute_returns(next_values, nstep_rewards, masks)\n",
    "            \n",
    "            agent.learn(values, log_probs, returns, entropy, optimizer)\n",
    "\n",
    "        episode_rewards.append(scores.mean())\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(episode_rewards)), end=\"\")\n",
    "\n",
    "        if episode >= 100:\n",
    "            mean_100_episode_reward = np.mean(episode_rewards)\n",
    "            epi_plot.append(mean_100_episode_reward)\n",
    "            if episode % 10 == 0:\n",
    "                print(\"Episode: {}, avg score: {:.1f}\".format(episode, mean_100_episode_reward))\n",
    "\n",
    "            if mean_100_episode_reward >= 500:\n",
    "                finish = True\n",
    "                print(\"Solved (1)!!!, Time : {:.2f}\".format(time.time() - start_time))\n",
    "                np.save(\"./single.npy\", np.array(epi_plot))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    print(\"Fail... Retry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -195.80Episode: 100, avg score: -195.8\n",
      "Episode 110\tAverage Score: -63.51Episode: 110, avg score: -63.5\n",
      "Episode 120\tAverage Score: 49.73Episode: 120, avg score: 49.7\n",
      "Episode 130\tAverage Score: 149.89Episode: 130, avg score: 149.9\n",
      "Episode 140\tAverage Score: 236.32Episode: 140, avg score: 236.3\n",
      "Episode 150\tAverage Score: 300.97Episode: 150, avg score: 301.0\n",
      "Episode 160\tAverage Score: 343.97Episode: 160, avg score: 344.0\n",
      "Episode 170\tAverage Score: 374.73Episode: 170, avg score: 374.7\n",
      "Episode 180\tAverage Score: 401.45Episode: 180, avg score: 401.5\n",
      "Episode 190\tAverage Score: 436.17Episode: 190, avg score: 436.2\n",
      "Episode 200\tAverage Score: 455.68Episode: 200, avg score: 455.7\n",
      "Episode 210\tAverage Score: 496.50Episode: 210, avg score: 496.5\n",
      "Episode 213\tAverage Score: 500.65Solved (1)!!!, Time : 1208.22\n",
      "Fail... Retry\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "\n",
    "def run(num_episodes):\n",
    "    n_steps = 5  # TODO up to you\n",
    "\n",
    "    Worker(num_episodes, n_steps)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-conversation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
